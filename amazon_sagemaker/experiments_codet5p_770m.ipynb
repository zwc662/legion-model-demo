{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Notebook on how to run inference using `CodeT5+ 770M`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25.1\n",
      "1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)\n",
    "#assert transformers.__version__ == \"4.12.3\", f\"wrong transformers version: {transformers.__version__}\"\n",
    "#assert \"1.9.1\" in torch.__version__  , f\"wrong torch version: {torch.__version__}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the `codet5p_770m` from s3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os, boto3\\ndef download_file_from_s3(bucket_name=None, src_loc=None, dest_loc=None):\\n    s3 = boto3.resource(\"s3\")\\n    bucket = s3.Bucket(bucket_name)\\n    for obj in bucket.objects.filter(Prefix=src_loc):\\n        target = obj.key if dest_loc is None             else os.path.join(dest_loc, os.path.relpath(obj.key, src_loc))\\n        if not os.path.exists(os.path.dirname(target)):\\n            os.makedirs(os.path.dirname(target))\\n        if obj.key[-1] == \\'/\\':\\n            continue\\n        bucket.download_file(obj.key, target)\\n\\ndownload_file_from_s3(\\'foundingblock\\', \\'vast.ai/60eps\\', \\'./codet5p_770m/\\')\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os, boto3\n",
    "def download_file_from_s3(bucket_name=None, src_loc=None, dest_loc=None):\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    for obj in bucket.objects.filter(Prefix=src_loc):\n",
    "        target = obj.key if dest_loc is None \\\n",
    "            else os.path.join(dest_loc, os.path.relpath(obj.key, src_loc))\n",
    "        if not os.path.exists(os.path.dirname(target)):\n",
    "            os.makedirs(os.path.dirname(target))\n",
    "        if obj.key[-1] == '/':\n",
    "            continue\n",
    "        bucket.download_file(obj.key, target)\n",
    "\n",
    "download_file_from_s3('foundingblock', 'vast.ai/60eps', './codet5p_770m/')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `codeT5+ 770M` using `torch.load`\n",
    "\n",
    "loading the model with `torch.load` took 7.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,T5ForConditionalGeneration\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load downloaded model and save as torch model\n",
    "model = T5ForConditionalGeneration.from_pretrained('./codet5p_770m/')\n",
    "torch.save(model, \"./codet5p_770m/model.pt\")\n",
    "tokenizer.save_pretrained('./codet5p_770m/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./codet5p_770m/model.pt\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./codet5p_770m/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "gen = pipeline(\"text2text-generation\",model=model,tokenizer=tokenizer,device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'SELECT song_name ,  song_release_year FROM singer ORDER BY age ASC'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = \"question: Show the name and the release year of the song by the youngest singer. schema:  | concert_singer | stadium : stadium_id, location, name, capacity, highest, lowest, average | singer : singer_id, name, country, song_name, song_release_year, age, is_male | concert : concert_id, concert_name, theme, stadium_id, year | singer_in_concert : concert_id, singer_id)\"\n",
    "\n",
    "gen(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test text2sql interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('code')\n",
    "from inference import model_fn\n",
    "generator = model_fn(\"./codet5p_770m/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text2sql.evaluation_examples import examples\n",
    "example = examples.examples[0]\n",
    "db_id, create_table_sql, question, query = list(example.values())\n",
    "inputs = [db_id, create_table_sql, question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./station_weather/station_weather.sqlite\n",
      "\n",
      "            PRAGMA foreign_keys = ON;\n",
      "\n",
      "\n",
      "            CREATE TABLE \"train\" (\n",
      "                \"id\" int,\n",
      "                \"train_number\" int,\n",
      "                \"name\" text,\n",
      "                \"origin\" text,\n",
      "                \"destination\" text,\n",
      "                \"time\" text,\n",
      "                \"interval\" text,\n",
      "                primary key (\"id\")\n",
      "            );\n",
      "table \"train\" already exists\n",
      "./station_weather/station_weather.sqlite SELECT LOCAL_Authorities ,  LOCAL_Services FROM station\n",
      "no such column: LOCAL_Authorities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'SELECT LOCAL_Authorities ,  LOCAL_Services FROM station',\n",
       "  'Result': None}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating `model.tar.gz` for sagemaker deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def compress(tar_dir=None,output_file=\"model.tar.gz\"):\n",
    "    with tarfile.open(output_file, \"w:gz\") as tar:\n",
    "        tar.add(tar_dir, arcname=os.path.sep)\n",
    "            \n",
    "\n",
    "import boto3\n",
    "\n",
    "def upload_file_to_s3(bucket_name=None,file_name=\"model.tar.gz\",key_prefix=\"\"):\n",
    "    s3 = boto3.resource('s3')\n",
    "    key_prefix_with_file_name = os.path.join(key_prefix,file_name)\n",
    "    s3.Bucket(bucket_name).upload_file(file_name, key_prefix_with_file_name)\n",
    "    return f\"s3://{bucket_name}/{key_prefix_with_file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from `./codet5p_770m/`\n",
      "saving model with `torch.save`\n",
      "saving tokenizer\n",
      "copying inference.py script\n",
      "creating `model.tar.gz` archive\n",
      "uploading `model.tar.gz` archive to s3://foundingblock/codet5p_770m/model.tar.gz\n",
      "Successfully uploaded to s3://foundingblock/codet5p_770m/model.tar.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://foundingblock/codet5p_770m/model.tar.gz'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil \n",
    "import tarfile\n",
    "import torch\n",
    "from transformers import AutoTokenizer,GPTJForCausalLM\n",
    "\n",
    "def compress(tar_dir=None,output_file=\"model.tar.gz\"):\n",
    "    with tarfile.open(output_file, \"w:gz\") as tar:\n",
    "        tar.add(tar_dir, arcname=os.path.sep)\n",
    "            \n",
    "\n",
    "import boto3\n",
    "\n",
    "def upload_file_to_s3(bucket_name=None,file_name=\"model.tar.gz\",key_prefix=\"\"):\n",
    "    s3 = boto3.resource('s3')\n",
    "    key_prefix_with_file_name = os.path.join(key_prefix,file_name)\n",
    "    s3.Bucket(bucket_name).upload_file(file_name, key_prefix_with_file_name)\n",
    "    return f\"s3://{bucket_name}/{key_prefix_with_file_name}\"\n",
    "\n",
    "bucket_name=\"model-stash\"\n",
    "key_prefix = \"codet5p_770m\"\n",
    "checkpoint = \"./codet5p_770m/\"\n",
    "model_save_dir = f\"./tmp_{key_prefix}\"\n",
    "src_inference_script = \"code\"\n",
    "dst_inference_script = os.path.join(model_save_dir, \"code\")\n",
    "\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "#os.makedirs(dst_inference_script, exist_ok=True)\n",
    "\n",
    "# load model\n",
    "print(\"Loading model from `./codet5p_770m/`\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "\n",
    "print(\"saving model with `torch.save`\")\n",
    "torch.save(model, os.path.join(model_save_dir, f\"model.pt\"))\n",
    "\n",
    "print(\"saving tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.save_pretrained(model_save_dir)\n",
    "\n",
    "# copy inference script\n",
    "print(\"copying 'code' directory\")\n",
    "shutil.copytree(src_inference_script, dst_inference_script)\n",
    "\n",
    "# create archive\n",
    "print(\"creating `model.tar.gz` archive\")\n",
    "compress(model_save_dir)\n",
    "\n",
    "# upload to s3\n",
    "print(\n",
    "    f\"uploading `model.tar.gz` archive to s3://{bucket_name}/{key_prefix}/model.tar.gz\"\n",
    ")\n",
    "model_uri = upload_file_to_s3(bucket_name=bucket_name, key_prefix=key_prefix)\n",
    "print(f\"Successfully uploaded to {model_uri}\")\n",
    "\n",
    "model_uri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) use **bash scripting** to upload compressed model file to the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (645606951.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    tar zcvf model.tar.gz *\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "tar zcvf model.tar.gz *\n",
    "aws s3 cp model.tar.gz s3://model-stash/codet5p_770m/model.tar.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (2.163.0)\n",
      "Requirement already satisfied: boto3 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (1.26.146)\n",
      "Requirement already satisfied: torch==1.13.1 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (1.13.1)\n",
      "Requirement already satisfied: transformers==4.26 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (4.26.0)\n",
      "Requirement already satisfied: protobuf==3.20.0 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (3.20.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from torch==1.13.1) (4.6.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from transformers==4.26) (2023.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from transformers==4.26) (23.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from transformers==4.26) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from transformers==4.26) (0.15.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from transformers==4.26) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from transformers==4.26) (6.0)\n",
      "Requirement already satisfied: filelock in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from transformers==4.26) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from transformers==4.26) (1.24.3)\n",
      "Requirement already satisfied: requests in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from transformers==4.26) (2.31.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: pandas in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from sagemaker) (2.0.2)\n",
      "Requirement already satisfied: pathos in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: jsonschema in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from sagemaker) (4.17.3)\n",
      "Requirement already satisfied: platformdirs in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from sagemaker) (3.5.1)\n",
      "Requirement already satisfied: tblib==1.7.0 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from boto3) (0.6.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.146 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from boto3) (1.29.146)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.146->boto3) (1.26.16)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.146->boto3) (2.8.2)\n",
      "Requirement already satisfied: fsspec in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26) (2023.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.15.0)\n",
      "Requirement already satisfied: six in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: dill>=0.3.6 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: pox>=0.3.2 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from requests->transformers==4.26) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from requests->transformers==4.26) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from requests->transformers==4.26) (3.1.0)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /Users/weichaozhou/Workspace/virtualenv_legionai/lib/python3.9/site-packages (from schema->sagemaker) (21.6.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Users/weichaozhou/Workspace/virtualenv_legionai/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install sagemaker boto3 torch==1.13.1 transformers==4.26 protobuf==3.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3\n",
    "\n",
    "iam_client = boto3.client('iam')\n",
    "role = iam_client.get_role(RoleName='AmazonSageMaker-ExecutionRole-20230622T164667')['Role']['Arn']\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint huggingface-pytorch-training-2023-06-23-20-07-53-554: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 22\u001b[0m\n\u001b[1;32m     11\u001b[0m huggingface_model \u001b[38;5;241m=\u001b[39m HuggingFaceModel(\n\u001b[1;32m     12\u001b[0m     image_uri \u001b[38;5;241m=\u001b[39m image_uri,\n\u001b[1;32m     13\u001b[0m     model_data\u001b[38;5;241m=\u001b[39mmodel_uri,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \trole\u001b[38;5;241m=\u001b[39mrole, \n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# deploy model to SageMaker Inference\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mhuggingface_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m\t\u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# number of instances\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m\t\u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml.m4.4xlarge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#'ml.p3.2xlarge' # ec2 instance type\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/virtualenv_legionai/lib/python3.9/site-packages/sagemaker/huggingface/model.py:311\u001b[0m, in \u001b[0;36mHuggingFaceModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_uri \u001b[38;5;129;01mand\u001b[39;00m instance_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m instance_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml.inf\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserving_image_uri(\n\u001b[1;32m    307\u001b[0m         region_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mboto_session\u001b[38;5;241m.\u001b[39mregion_name,\n\u001b[1;32m    308\u001b[0m         instance_type\u001b[38;5;241m=\u001b[39minstance_type,\n\u001b[1;32m    309\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHuggingFaceModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvolume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvolume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_data_download_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_data_download_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_recommendation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_recommendation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/virtualenv_legionai/lib/python3.9/site-packages/sagemaker/model.py:1328\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_explainer_enabled:\n\u001b[1;32m   1326\u001b[0m     explainer_config_dict \u001b[38;5;241m=\u001b[39m explainer_config\u001b[38;5;241m.\u001b[39m_to_request_dict()\n\u001b[0;32m-> 1328\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_from_production_variants\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproduction_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mproduction_variant\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls:\n\u001b[1;32m   1340\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session)\n",
      "File \u001b[0;32m~/Workspace/virtualenv_legionai/lib/python3.9/site-packages/sagemaker/session.py:4577\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict)\u001b[0m\n\u001b[1;32m   4574\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating endpoint-config with name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[1;32m   4575\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_endpoint_config(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_options)\n\u001b[0;32m-> 4577\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/virtualenv_legionai/lib/python3.9/site-packages/sagemaker/session.py:3970\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   3966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_endpoint(\n\u001b[1;32m   3967\u001b[0m     EndpointName\u001b[38;5;241m=\u001b[39mendpoint_name, EndpointConfigName\u001b[38;5;241m=\u001b[39mconfig_name, Tags\u001b[38;5;241m=\u001b[39mtags\n\u001b[1;32m   3968\u001b[0m )\n\u001b[1;32m   3969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 3970\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m endpoint_name\n",
      "File \u001b[0;32m~/Workspace/virtualenv_legionai/lib/python3.9/site-packages/sagemaker/session.py:4322\u001b[0m, in \u001b[0;36mSession.wait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   4316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   4317\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   4318\u001b[0m             message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   4319\u001b[0m             allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   4320\u001b[0m             actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   4321\u001b[0m         )\n\u001b[0;32m-> 4322\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   4323\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   4324\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   4325\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   4326\u001b[0m     )\n\u001b[1;32m   4327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m desc\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint huggingface-pytorch-training-2023-06-23-20-07-53-554: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.."
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"]=\"us-east-2\"\n",
    " \n",
    "model_uri=\"s3://model-stash/codet5p_770m/model.tar.gz\"\n",
    " \n",
    "image_uri=\"763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04\"\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    image_uri = image_uri,\n",
    "    model_data=model_uri,\n",
    "\t#transformers_version='4.26',\n",
    "\t#pytorch_version='1.13.1',\n",
    "\t#py_version='py39',\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.m4.4xlarge'  # ec2 instance type\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text2sql.evaluation_examples import examples\n",
    "example = examples.examples[0]\n",
    "db_id, create_table_sql, question, query = list(example.values())\n",
    "inputs = [db_id, create_table_sql, question]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor.predict({\n",
    "\t'inputs': inputs\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameterized request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Can you please let us know more details about your \\nissue?\\n\\nA:\\n\\nThe problem was caused by my lack of understanding on how web sockets \\n  worked. Once I understood how they work; I was able to fix'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict({\n",
    "\t'inputs': \"Can you please let us know more details about your \",\n",
    "  \"parameters\" : {\n",
    "    \"min_length\": 120,\n",
    "    \"temperature\": 0.9,\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "custom end of sequence token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "end_sequence=\".\"\n",
    "temparature=40\n",
    "max_generated_token_length=50\n",
    "input=\"Can you please let us know more details about your \"\n",
    "\n",
    "predictor.predict({\n",
    "\t'inputs': input,\n",
    "  \"parameters\" : {\n",
    "    \"min_length\": int(len(input) + max_generated_token_length),\n",
    "    \"temperature\":temparature,\n",
    "    \"eos_token_id\": tokenizer.convert_tokens_to_ids(end_sequence)\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
